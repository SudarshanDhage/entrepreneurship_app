# 50 Short Product Lessons

A comprehensive collection of product development lessons, organized by theme. These lessons were compiled from brief talks, so the tone is conversational.

## Table of Contents

1. [Bets](#bets)
2. [Opportunities vs Interventions](#opportunities-vs-interventions)
3. [Premature Convergence](#premature-convergence)
4. [Mission vs Projects](#mission-vs-projects)
5. [Starting Together](#starting-together)
6. [Coherence and the Messy Middle](#coherence-and-the-messy-middle)
7. [Multiple Operating Models](#multiple-operating-models)
8. [Data as a Trust Proxy](#data-as-a-trust-proxy)
9. [Chronic vs Acute Issues](#chronic-vs-acute-issues)
10. [The Pull of Short-Term Thinking](#the-pull-of-short-term-thinking)
11. [The Time It Takes to Get Good](#the-time-it-takes-to-get-good)
12. [Play Less Tetris](#play-less-tetris)
13. [Moving Fast and Slow](#moving-fast-and-slow)
14. [Storytelling, Repeated Stories](#storytelling-repeated-stories)
15. [Strategy as It Relates to Beliefs](#strategy-as-it-relates-to-beliefs)
16. [Prioritizing Opportunities](#prioritizing-opportunities)
17. [Learning Cadence](#learning-cadence)
18. [Learned Helplessness](#learned-helplessness)
19. [Data Snacking vs Integrated Approach](#data-snacking-vs-integrated-approach)
20. [Functional Feature Factories](#functional-feature-factories)
21. [Left to Right Factory Lines](#left-to-right-factory-lines)
22. [Modeling the Business](#modeling-the-business)
23. [Teams Closer to Customers](#teams-closer-to-customers)
24. ["Weird" Practices](#weird-practices)
25. [One Pagers](#one-pagers)
26. [Not Dividing Out Customer Facing and Non Customer Facing](#not-dividing-out-customer-facing-and-non-customer-facing)
27. [Leaving Time to Iterate](#leaving-time-to-iterate)
28. [PMs Creating an Environment for Good Decisions](#pms-creating-an-environment-for-good-decisions)
29. [Two Priority Levels](#two-priority-levels)
30. [Bring Me Solutions and My Idea-ism](#bring-me-solutions-and-my-idea-ism)
31. [Safe Places to Workshop Ideas/Bets](#safe-places-to-workshop-ideabets)
32. [The Critical Moment of Realizing Intuition Needs Testing](#the-critical-moment-of-realizing-intuition-needs-testing)
33. [Credibility by Not Manufacturing Certainty](#credibility-by-not-manufacturing-certainty)
34. [Artificial Deadlines](#artificial-deadlines)
35. [Forcing Functions Are Valuable](#forcing-functions-are-valuable)
36. [Not Hiding Dependencies](#not-hiding-dependencies)
37. [Products and Features Are Temporary](#products-and-features-are-temporary)
38. [Shifts That Necessitate This All](#shifts-that-necessitate-this-all)
39. [Expertise as a Service](#expertise-as-a-service)
40. [Perspectives on Roadmapping](#perspectives-on-roadmapping)
41. [Measurement to Inform Decisions](#measurement-to-inform-decisions)
42. [The Reflection Is What Counts](#the-reflection-is-what-counts)
43. [Shared Understanding / Vocabulary Is Super Hard](#shared-understanding--vocabulary-is-super-hard)
44. [Prioritization Spreadsheets](#prioritization-spreadsheets)
45. [A Longer-Term View of Customers](#a-longer-term-view-of-customers)
46. [Product Strategy Part 2 - Don't Outsource It](#product-strategy-part-2---dont-outsource-it)
47. [Connected / Embedded Design](#connected--embedded-design)

---

## Bets

One interesting thing about bets is that they come in all different sizes. You can have big bets, small bets, safe bets, or risky bets. There is also an element of time: bets can have different durations. You can have really, really short bets that might be short, but still very large. And you can have really long-evolving bets that might take a very long time to mature. At any given time, a company will have a portfolio of bets developing in parallel that are all interlinked and related in some way.

You might ask, "Why is it important to think in terms of bets?" The simplistic answer is, "We're making an investment, and we want a really good return on that investment." That is true, but the usefulness of thinking in terms of bets is that the different types of bets, the shape of bets, and the relatedness of bets are very pertinent to product development. There are helpful mental models for what we're doing.

Some bets take a long time to mature. A startup might be based on one, two or three core fundamental ideas. Although you want to update your beliefs as you start to accumulate data, some things just take a while to mature. It takes you time to understand what's actually going on. That's a really good example of how thinking about those fundamental aspects of your company and making those very clear to the people you're working with can both inspire them to help you update your prior understanding—what information you have relating to those bets—but also help you to communicate the real direction.

A classic problem when someone joins a startup is that they come in and appraise what's going on and say, "Why do we believe that? It seems a little risky. Should we challenge that idea? Should we challenge that other idea?" Often, what they don't realize is that people have gone through that particular thought process and have decided where to make operating assumptions and have decided if these are bets that they want to play. They've decided what they know to be risky and what they know to be safe for this case.

When you can communicate those bets and beliefs to someone who is just joining the company, you save them the hassle of not understanding the thought process of the people who founded the company. The other important thing when you think in terms of bets—as anyone who plays games of skill that involve betting knows—is that there are better ways to place your bets. There are often better ways to play those games. If we have an opportunity to create a game where we can bet more incrementally instead of a big batch bet, of course we're going to try to create an environment where that is possible.

Some things don't work like that. Sometimes we don't have that opportunity, but if we at least understand the game and the bets we're making, it challenges us to think about the potential for changing how we play that particular game. Another key benefit is that it's tempting to try to take a one-size-fits-all approach to all the work we're doing. And just when we try to think about the various bets that we make in our lives or in other areas, we realize that there's an almost infinite number of variations of these styles of bets.

Yes, they can be sorted into a few categories, and there are some models that encourage you to think about certain categories of bets for your company. But it really makes you aware that you need an adaptive approach to working on these things, otherwise you're going to try to treat everything in the exact same way. Underscoring this idea of the interrelatedness of your bets and their circumstances is that you can have core beliefs that then filter all the way up into the work that's happening right now. You can have bets across one to three decades, or a series of bets and beliefs that impact the work that you're doing right now. That interrelatedness is extremely important to consider.

---

## Opportunities vs Interventions

One challenge with dividing things into problems versus solutions is something that anyone who has worked in product development understands: almost every problem is a nested solution to a higher-level problem. For example, when I have issues with how we're going to generate revenue for the company, that's a problem. But it's also a solution to some other higher-level mission for the company about why you exist, the change that you hope to engender, or how you want to create long-term growth and value for your company. Similarly, a low-level issue could involve improving the performance of a particular page. That's also a problem, and it's a perfectly compelling and interesting problem, but it's actually a solution for some higher level goal that you're pursuing.

The challenge is that organizations often struggle to define who owns problems and who owns solutions. The debate will never be resolved completely because at its heart, it's a debate over how things are decomposed. It's a debate over who decides how those things are all linked together. One test I use to tease out a problem's full connection to what's going on within the company is to imagine some really smart person in the back of the room continually asking, "Why," or "What are we hoping to achieve with this?"

Now, opportunities and interventions are a little different, because the word intervention implies that we're somehow doing something that may change behavior for our benefit or that may actually change behavior in a way that we didn't expect—or which might not even be beneficial for the company. And it implies a temporary nature to our efforts. It implies that we might decide not to continue intervening that way. It's a useful framing for how people think about delivering features. They imagine a level of permanence to what they're doing, that it's going to stick around forever and that they need to perfect what they're putting out there in the world. That kind of pressure on what you're doing causes a lot of problems.

Similarly, with any product, there is guaranteed to be a steady stream of problems to solve, but just because a problem exists (or just because customers are complaining about something), this doesn't necessarily mean that there's value in solving that problem.

You also expose this by framing things as opportunities. For example, we have an opportunity to make customers more successful at something, or we have an opportunity to unlock this part of the market. Yes, you could also frame that as a "problem." However, this lens reveals that there's not really one underlying binary problem that is either solved or isn't solved. Perhaps you've have or haven't successfully realized opportunities or explored a certain area. We learn more about the opportunity as we dig into it. You can incrementally capture value from a particular opportunity. Those are characteristics of problems that are hard to talk about. And things will also pop up that don't strike anyone as a genuine problem.

Let's take a not uncommon example: a status quo exists, and people are generally happy with how things are. You could look at this and think there's not really much you can do there. Now, opportunities imply that there may be ways shift the paradigm from how you're currently working.

Thinking about prioritizing problems to solve, it actually feels a little different to when you're prioritizing opportunities to capture additional value. This perspective tends to inspire people to think a little bit more broadly about what they're doing and can actually inspire people to be a little bit more strategic about what they're doing. Framing situations in terms of opportunities and interventions can go a really long way in terms of getting your team to be more impact-focused.

---

## Premature Convergence

So what is premature convergence? Premature convergence is trying to zero in on a solution (or decision) earlier rather than later, potentially too early. Now, this can obviously be a little tough to judge. There's an ideal spot that that's probably a range for most of your efforts. But the really important thing is that our intuition often drives us to converge on a solution earlier than would be optimal. This happens because we don't like uncertainty. We want plans, we want to be able to say exactly what we're going to do.

Uncertainty is often not highly regarded in a company environment. People are rewarded for having a very specific plan and being able to say specifically what they're going to do. The other temptation is that people want to keep a team really busy, or they want to make sure that there's something teed up ready for the team to jump into next. There's pressure on them to formulate those plans and lock down what those plans are earlier rather than later.

All the reasons why you want to converge earlier are very clear, but all the costs of doing that are often not very visible. So unless you've actually experienced it and experienced the benefits of waiting to converge—or of allowing a period of messy reality—it's unlikely that you're going to really see the net benefit.

Often, when you converge later, you realize you're solving the wrong problem. You have an opportunity to gather diverse perspectives, you get more people seeing the problem for the first time and experiencing the problem for the first time. This is difficult when you just drop things on a team out of the blue. They don't have that experience of grappling with the problem for the first time. They might not think about creative ways of solving that particular issue. And the people who have the idea are often subject to a lot of confirmation bias and sunk cost bias. They've invested a lot of time in coming up with that particular solution or converging on that particular problem. Now, even though we know this, it comes up again and again and again, and it's tough to even call it an antipattern because there's so many near-term reasons why we think it's right to do that, that it's almost an intuition trap.

So when is the right time to converge? It's easy for me to say, but I think the answer is "a little later than is comfortable." This idea is borne out as you look at a cross-section of teams. When you're converging at the right time, there is a period of messiness, there is a period that includes a little bit of discomfort. This is how you know you're on the right track. When you've converged a little too early, what you'll typically observe is near-term speed and efficiency by jumping into the problem, a kind of near-term momentum. But often just a very short amount of time into the effort, it really becomes clear. Maybe you started off on the wrong track, or it's very hard for people to communicate all the requisite contexts that they have because they converged early. You'll experience a big hiccup. That's one way of recognizing that you've converged a little too early: look for that initial sense of certainty. It's usually a pretty good sign of what's happening.

One additional thing to be on the lookout for is that when things are moving slowly, there's often a heavy, heavy urge to figure out all of this stuff upstream. This is particularly strong when people are twiddling their thumbs because there's not a lot of flow in the system. People tend to converge more and more on plans. You should resist these impulses; the whole idea is to plan at the last responsible moment instead of planning instinctively because you're so nervous that things aren't happening right now.

---

## Mission vs Projects

There have been a couple really amazing talks recently about product thinking versus project thinking, and I think that those discussions are extremely valuable. It's very important to understand the difference between what a project is and what a product is. But I don't actually think the comparison is apples to apples. It's a challenge in the sense that when you are iterating on or offering a product, there tend to be initiatives or missions baked into doing that. You could make a reasonable argument that a product is the byproduct of a series of projects that have been brought to completion. That is a reasonable argument to make, although there are some important differences between project thinking and product-oriented thinking.

I like to think about missions or initiatives, and how those differ from projects. They do differ significantly from projects in the sense that they can be open ended. They don't necessarily end with delivery. Which isn't to say that all projects are like that, but that is a common framing of a project. The longer that you spend on a mission, the greater the suggestion that that particular mission is valuable, that you're having success improving a particular metric or improving outcomes for your users, or accomplishing any number of things.

And that's a huge difference to how most people think. Most people think that the quicker you get things done, the better. In mission-oriented thinking, yes, you want to learn quickly, and you want to quickly figure out how to offer more and more value. But you're not constrained by this idea and to the factory metaphor of delivery—just dropping things off the end of the assembly line. Instead, frame things as bets as I discussed in the bet section.

Missions are also often nested. There are really small missions that might take a couple of days, and those that in some way feed into or are linked to larger missions that might take a couple of months, a couple of quarters or even a couple of years. The whole company is based on a series of missions, just like it's based on a series of bets and beliefs.

The important framing is that a mission might also have a stopping function. The team might have an agreement about when they will decide that pursuing this mission any further might not be beneficial. That is very, very different than a predetermined definition of done, a delivery or state you achieve that clearly determines the endpoint. An example of a stopping function might be when the rate that we're able to improve what we're working on drops below a certain threshold. At that point, we might decide to reconsider. We might want to consider stopping, pivoting, or embarking upon a different approach.

An argument could be made that this is just semantics, that you can obviously just mold the project idea to encompass a definition of done for some kind of outcome. But I think that philosophically, what you're talking about is shaping an approach that people enjoy. I think that humans do enjoy this idea of the initiative for the mission, and products risk devolving into a feeling of endless iteration. Having a container for an initiative does make sense on a human level. Reframing the idea of a predetermined outcome for the effort into improving someone's life or improving a particular metric or entering your particular market can be extremely powerful.

---

## Starting Together

Starting together is something that I've spoken and written about a great deal. The whole idea of starting together is that there is a tendency to send people upstream or to have smaller groups of people initiate work. And the fascinating thing is, if you ask a team what work that they have in progress, they'll often show you some set of work. But when you ask what the people are actually working on—and I mean everyone, what is everyone working on—you'll hear about lead architects being in meetings for something that's supposed to happen in eight months. There will be PMs and user experience meeting for things that have potential. You often find that the amount of work that's theoretically in progress is actually dwarfed by all of this planning and decomposing and pitching and discussing. The whole idea of starting together is trying to limit that planning inventory, trying to really kick off an effort with all of the people involved, and striving to minimize premature convergence.

This doesn't preclude people trying to build context around something or understand the size of an opportunity or to set context. What it does mean is that instead of a small number of people dropping it on the team, you try to get the whole team experiencing the problem for the first time. I like to use an analogy from movies when the group of friends opens the door of a haunted house together and you see all of their eyes go really, really wide. That's the sign that they're experiencing the problem for the first time. You see a lot of movies where there's a small group of people involved in the first 10 to 20 minutes of the movie. Gradually, they assemble the whole team after they've endured some trials and tribulations in the beginning. That's the point where you get the team preparation montage, the tension builds a little bit, and they get all of these unique ideas and ways to solve the problem because the team has been assembled. That's the kind of starting together that I'm talking about, because what you find in those movies is that a lot of that initial battling doesn't necessarily equate to the power of the team once the team has actually assembled.

The whole idea of starting together is to figure out how you can plan a sequence, how you can arrange and construct your teams in a way that they can truly start working on something and clear their calendars. This is not, "Oh we do one meeting in the morning and then we continue to do business as usual," it's really just clearing their calendars so that they can all jump into this problem together.

And what might this look like? There might be customer interviews or joint research activities. You might have various people presenting bits of data that might be known about the particular problem. You might do customer visits. You might try to start doing some rapid prototyping. There are many different paths a team might take, and it's difficult. You don't want to adopt a one-size-fits-all approach to starting together—that's not really recommended for anything. But the whole idea is getting everyone in the room, clearing out the time, and doing some kind of kickoff that really aligns people on the opportunity. Then you experience that initial exploration together, as a team.

When I see a lot of team struggles, especially in terms of getting things done, you can often trace it back to the kickoff. You can often trace it back to there being a lack of alignment. That might not even be the word you're looking for, because poor alignment at the beginning is quite common, but if everyone is experiencing that at the same time, then when you actually do get to the point of a deeper level of alignment, the result is all the more powerful. Generally, the idea is to create an environment in which the team can experience the problem for the first time together and connect with the customer.

---

## Coherence and the Messy Middle

One thing that you find when you talk to teams is that it is often very difficult for the people working on the front lines to connect their low-level work all the way up to the larger bets of the company. You find that the near-term work in the one to three-week, or even the one to three-month range, is usually pretty known because that's what's dictating people's lives.

The larger company bets, the one to three-year bets are also known, but they're almost by definition somewhat vague. They're very directional. They don't feel real at that moment. They're hard for people to wrap their heads around. And then there's a middle layer, which I like to call the "messy middle," which is neither short-term work nor really long-term work. It's those bets, the things at that level that teams often have the hardest time wrapping their heads around. They have difficulty conceptualizing how their work fits into that messy middle work, and how that work fits into larger long-term missions.

One of the reasons that this happens is simple: we spend a lot of time talking about short-term work and the large, long-term things. But once the bets have been made, often, that one to three-quarters of the work is not really discussed all that much. It's only mentioned if you're being very intentional about it. The key to building this kind of coherence is that you need to keep reiterating the thought behind the bets in that middle range: what you know about them, your progress towards resolving those particular bets. Have your beliefs changed? How and what are the teams doing? And even how those bets are related to the high-level goals of the company.

One thing that's misunderstood is that people believe that you need certainty to have coherence. That's not what you need. Coherence is the ability to be able to navigate and link together the work that you're doing in all of its messiness and all of its uncertainty and just tell a coherent or persuasive or connected story about how this relates to the larger things that you're doing as a company. People often chase certainty, and the problem is that when you manufacture a certainty, you actually limit coherence.

---

## Multiple Operating Models at Once

One thing that becomes abundantly clear when you talk to teams is that there are often many different types of bets in progress at once. In fact, you want a balanced portfolio of bets, which means that you're going to have teams tackling very different types of work simultaneously. An antipattern you observe is that some form of process is expected to apply to all the types of bets in your portfolio, and that somehow you'll find some one-size-fits-all approach that will work for that.

One thing you observe with higher-performing teams is that they tend to have a relatively stable set of patterns for how they approach all of the things they do, while also being able to absorb the unique nature of the different types of work that they're doing.

One great example would be where some companies get a lot of small customer feature requests which, if they could just knock them out, would have a relatively low blast radius. They're pretty easy. They make sense. How the team approaches working through those is very different from how we'll approach more exploratory efforts. This requires an approach that will accommodate both.

---

## Data as a Trust Proxy

One thing that you observe as teams grapple with using data, measurement, insights, or any number of things, is that there's often this idea that data will serve as some kind of trust proxy. That somehow a piece of data will settle all the ties in opinion, that the highest paid person will suddenly stop enforcing their opinion with other people, that suddenly you'll have all this certainty about what you're doing.

What this misses is that when you see teams making effective use of data in their environments, there's often a lot more uncertainty. There are a lot more questions about why what you're doing isn't working. There are wide spectrums of confidence. When you initially embark on some effort, you only have rough assumptions that you've baked into the metrics that you're tracking or the analysis that you're doing. Over time, your confidence increases, but it is an ongoing effort. You have to renounce the idea that the data will pass/fail your teams or will once and for all tell you if you're moving in the right direction. Instead, it's important to adopt a learning stance, one that acknowledges measurement as a catalyst for learning.

---

## Chronic vs Acute Issues

I'm always struck when people read a blog post from a popular company and then say things like, "It's perfect there, look at how well that's working," and "Why can't we work like that?" As someone now who's managed to talk to a lot of those companies, one thing I can say for sure is that it doesn't get easier. Maybe you learn faster or you go faster or have more impact. But a lot of the acute challenges that arise happen in those particular teams. Interestingly, I would say that the difference is that there tend to be fewer chronic problems there. Such organizations find a way to be resilient and to prevent things from becoming chronic and to solve more acute issues.

You'll talk to an executive who'll say, "We've got all these problems, but we tend to just knock out the ones that are really limiting us that people bring up." And if you talk to the employees on those teams, they'll say the same thing. They'll say, "Nothing's perfect here. We've got our fair share of problems. Two years ago, we encountered that, and we kind of worked it out. And a year and a half ago, we encountered this other thing. We worked it out." These companies have a sense of self repair, a sort of immune system.

Now, compare that case to companies that seem to be struggling with a lot of chronic issues: companies that are carrying a lot of debt, or where it might take years for a toxic person to be let go, or that have been affected by a merger or an acquisition. From the outside, there's often an impression that everything is rosy, but that's just not the case. What distinguishes them is this element of self repair.

---

## The Pull of Short-Term Thinking

It's almost impossible to find companies that aren't struggling with the tension between mid and long-term outcomes (and upside and potential) and the pull of short-term demands. Any company that says that they've solved this problem is probably lying; it's always a balancing act.

There are a couple of solutions to this. One is to be very stubborn. Many people who've had multiple failures in their careers start to develop a level of stubbornness about what they're doing. This often relates to not chasing short-term outcomes. It's a level of stubbornness related to how they want their company to be—leaps of faith that defy the short-term—that they feel will build a more resilient company in the long run.

The other opportunity is to compress the durations of activities. The standard problem with these short-term gains is that they can generate negative side-effects. If you try to build quickly, and in the process introduce a lot of technical debt into your product while chasing some short-term outcome, you will only feel the impact of those decisions after a certain amount of time has passed. This lag time impacts your ability to course correct. One option is to become better at sensing the early indicators that you're going off the rails, and then feed that back into the decision-making mechanism.

---

## The Time It Takes to Get Good

It does take time to get good at this—or to get good at anything. But you observe again and again that companies trying some new process or technique for the first time often incorrectly expect to see immediate results and to be good at it instantly.

I always like to mention that I worked in a company environment that was really pretty healthy. It was a great product, and there was a good attitude towards product. New junior people like designers, engineers and product people would join the company straight out of college. Full onboarding took time: it took as much as 12 to 18 months to get them to the point where they were really starting to grasp being given a more open-ended opportunity and had the ability to extract value from it.

This allowed them to really come into their own as product developers. Not only that, it wasn't just the 12 to 18 months, it was repetition. It wasn't just one big effort. They went through the mission cycle. They went through the cycle of encountering a new problem and tackling that problem, over and over again. And importantly, they were allowed to fail. Early on, they weren't all that great at it. But instead of someone coming in and simply saying, "Well, you guys are doing it all wrong, and this is exactly how to do it," there was a level of safety at the company where teams that were wandering off in the wrong direction were allowed to wander off in the wrong direction.

---

## Play Less Tetris

You end up meeting a lot of product managers who consciously or unconsciously perceive part of their job as loading up teams, engaging in what I view as an elaborate form of Tetris. They're looking at individuals and saying, "What are they doing right now? Maybe they could take these three other things, or maybe that other team could split things up five ways—20% each, or when that engineer's done with that one thing, I've got this next thing that I'm going to load up on them."

One of the underlying components of this is the idea that it's part of their job to fill people up, to get more output. This is not limited to just product managers. You see this with engineering managers a lot. You see this with individuals a lot; as individuals, we also try to play Tetris with our time.

The important side effect of playing Tetris is that it leaves less time for experimentation and less time for exploration, etc. When we believe it's our job to keep people busy, we tend to pre-converge on things, and we tend to rush things so we can drop them on teams. We allow less time for teams to truly start together, and we engage in dependency wrangling.

And as it relates to measurement: if everything is predetermined in the game of Tetris—the puzzle pieces have all been placed—you won't have the leeway to iterate on things or to explore options. You're going to be locked into a particular plan because you will have over-constrained yourself so much with all these commitments and individual backlogs and other things that you've put together.

---

## Moving Fast and Slow

A lot of organizations place a heavy emphasis on moving quickly and on output. You even see some pretty popular companies brag about how many features they managed to complete in a certain period of time. Maybe that's good for those companies.

But there is a contrary movement. Designers and architects are often associated with the idea of questioning this pressure to move because it involves cutting so many corners. "We're putting crap out there—why aren't we taking our time? Do we actually need to deliver this quickly?"

The arguments are polar opposites. Teams that appear to be doing well are actually able to incorporate these impulses in a way that results in a bias for frequent integration. By that, I mean a bias for learning, for integrating assumptions and then testing them, and for making sure that they don't go too far off course chasing some silver bullet. In fact, there is a bias for shipping or testing—for action. At the same time, there is longer-term thinking involved, a habit of leaving room to iterate on something and to explore. These teams allot time to "bake in" the product and investigate whether it's working or not. This is a more deliberate approach.

A classic example is a team that's shipping quickly. The flip side of that is a team that is shipping pretty quickly, but which is also learning as fast as they are shipping, as one coworker put it: they've harmonized learning and shipping.

---

## Storytelling, Repeated Stories

One thing that becomes clear when you talk to teams that you know are doing well (i.e. increasing their market share and making humans love their product and all those types of things): they tell very coherent stories about how they work, and the way they describe how they work is very disciplined.

When you ask someone, "How's it going? What are some recent product decisions? What is your product strategy?" a less experienced person will say, "We're working on this right now." But there's not a lot of context around that.

When you talk to someone who is really on top of their game, they will paint the whole picture and how what they're working on fits into the broader story. They'll say something like, "In our company, there are three major forces that we think will shape the market for the next six years. This is one big unknown, and here's another. Our unique edge on this is that, and the way this translates to our six-month areas of focus involves three primary puzzles that we're grappling with," etc. That will continue until you are given a coherent, holistic picture of the situation.

This is important, as it is when they talk about their recent product efforts, because there's a depth to their explanations. They are discussing their assumptions. They are talking about what they thought happened. They are talking about what they learned and what surprised them. They're talking about specifics: details and data—both qualitative and quantitative. And they're telling good, meaningful stories, not vague stories about what happened in the last six months.

---

## Strategy as It Relates to Beliefs

Especially in teams that have really begun to embrace the idea of working very iteratively, of working with sprints or other similar things, strategy can often carry a negative connotation. It's big, and there are a lot of assumptions. It's something that executives put in PowerPoint decks. It's very tired, and it's a lot of talk.

But it boils down to other things that we've discussed: there are a lot of product teams that almost have their hands tied, which doesn't make sense. Another way to put this is that their future is largely dictated by a number of decisions about which markets to enter, which personas to target, where they think the market is going, what competitors are doing, and what's going on in general.

What I like to ask teams is, "Which wave are you riding?" You see certain problems that over the years, multiple waves of people have been trying to solve. Some companies get in early and then perhaps become too big to innovate. Some people get in later and have the benefit of access to new technologies, but they can't really compete. Some people enter later thinking that the problem is really X, and then the whole industry is turned upside down by some form of disruption.

This isn't a suggestion that you just need a bunch of PowerPoint decks with lofty projections on the whole space. But it is important to lay out and map the core beliefs of where you think things in your industry or space will move. You should identify where you are relative to legacy solutions, relative to your potential disruptors and relative to your current competitors—the whole stack of what you do.

---

## Prioritizing Opportunities

Prioritization is a huge topic and there are many approaches to thinking about it. One pattern that I see repeatedly, which is troubling and which almost serves as a blinder for teams, is a common way to think about prioritization: value and effort. Is this high-value, high-effort is a very simplistic way to think about it. It's very simplistic because often the most valuable things you're working on are huge multiples more valuable than the low-value things you're working on. In terms of effort, there can be a wide range as well. It doesn't really take into account things that are more friendly to experimentation, where you can iterate your way through. There is an area of confidence, and there's risk.

But if you step way back, I can paint you a picture. As a company, we may believe that there is a big opportunity if we really made a certain actor or persona a lot more efficient in the way they work. We think there's a huge opportunity there if we can accomplish that. And there is a whole plethora of ways that we could try to make that happen.

Some of those are big lifts, and some are small lifts. There's just a whole variety of ways that you can move them. But if we also consider our product strategy, that opportunity is by far the biggest opportunity that we can work on. Now, this ties together with some other things we've talked about, about premature convergence and starting together. The tendency at that point is to say, "What are we going to do to exploit that opportunity?" Someone will say, "This is low hanging fruit. There are other things that we could take care of. Here's a level of effort." The prioritization goes like that. Things are then sequenced based on that opportunity size, adjusted by effort.

Now the danger at that point is that we often forget how important that opportunity is. So instead of saying to a team, "There's this huge opportunity. We trust you. There might be some small stuff or some big stuff, but whatever you do, if you can just keep it pretty snappy and experimental in the beginning, it's going to be good for us. We just need to explore and extract that opportunity." Some team has already begun discussing who might tackle a thing, and we'll tackle this other thing. Or they've committed to doing this particular item.

The important point here is that when you're prioritizing, product should really consider prioritizing by the size of the opportunity and try to resist taking on too many opportunities at once.

---

## Learning Cadence

We have touched on this in some of the other sections, but I wanted to zero in on the idea of learning cadence and to differentiate that from a shipping cadence or delivery cadence. The best way to tackle this is to imagine that you have asked a team to talk about a handful of things each week that it learned about customers or users. What would they talk about? What would be the volume of that learning? What would be the depth of that learning, and how might it shape what the team is doing right now?

A question that I ask teams a lot is "What have the big Aha! moments been that really created a pivot for you, that really forced you to rethink how you're approaching what you're doing?"

The variety of responses to that question is amazing. Sometimes it's, "About six months ago we changed our strategy a little bit based on some learnings from maybe eight months ago, but now we've been pretty much in execution mode, and we're just rolling through and moving on that." And then you get other teams that say, "Wow, I can't even begin to count the number of things that we've learned in the last six months. We've learned that users did this. We made this mistake. We learned this from someone else, and we've been getting feedback from the market that this other thing is happening, while also tweaking something else."

This learning velocity is a really powerful way to understand what's happening and how your team is working now. Often, when a team or a small group of designers or strategists has a fair amount of upfront research, they're doing a lot of learning. There's rapid learning every day. With every customer conversation, you're learning something, and then you see this shift: you've got that learning, and now you're trying to exploit it in a different context.

I don't think there's anything inherently wrong with that, and that certainly makes some sense. But you have to contrast that with teams that may do a bit of that deeper upfront learning, where they're always revisiting those assumptions. They're revisiting to see if they are on the right track. And probably most importantly, they're acting on that learning.

---

## Learned Helplessness

Sometimes, when I'm talking to a product manager, they'll say something like, "I wish my team would be more interested in research and exploring the problem. I'm not sure why they aren't."

This is a really interesting question because people certainly have a range of interests. I have engineer friends that say, "It's not my job to figure out what we need to build. My job is build." I respect where they're coming from. I have other friends who are engineers who say, "I've kind of given up on what our PM does. I can't make heads or tails of it. So I'm just checking out. I just want to focus on the technology. I just want to focus on this. And honestly, I don't have a lot of confidence in what they're doing, but it's just better this way. I don't get involved in what they're doing."

That's a bit different, right? That suggests that they're interested, but they've probably gotten burned a couple times. You find these attitudes in environments with incredibly low psychological safety, where engineers and designers, etc. are interested, they want to discover the problem, and they want to have more impact. But they just don't. Either they aren't enabled or empowered to go upstream and get involved, or perhaps they got involved in the past and got swatted down for doing that. There's just this harsh wall that exists.

The challenge, I think, is that I've observed teams that just become really evolved after practicing a lot. In those teams, designers and engineers can pretty much do almost all of the activities generally associated with a product manager, which frees up that product manager to be more strategic and think about other things. It's wonderful to see teams where that has happened.

But if you as a product manager get into the habit of just making these kinds of overly prescriptive statements, the team will adapt and optimize around you doing that. They're going to optimize on you trying to put this kind of solution on the plate. As a result, even when they want to get involved, they haven't really practiced enough, and it's all a very new experience to them.

---

## Data Snacking vs Integrated Approach

In these conversations, you often notice a difference between what I would term "data snacking," and a more integrated approach. In the first, people cherry pick data to support a particular effort that they're engaged in, or perhaps to answer one particular question. Not that there's anything wrong with answering questions, but here, the whole idea is that insights serve the purpose of occasionally agreeing with and supporting what you're doing at the moment.

By contrast, what you notice with teams that are making better use of data (and measurement and insight), is that data is integrated into many different facets of product development. This isn't to say that these teams are completely data-driven, rather that qualitative and quantitative data is woven into the fabric of all of their various efforts as a team.

For example, in kickoffs, you'll see context being presented as data. You'll see data about the problem, and as the team presents it strategy, you might see the strategy represented as a model of particular metrics or beliefs. This can be supported by qualitative data. As the team is reflecting back on what it's doing or did the last quarter, the metrics that they're using provide context.

The important thing is the presence of a consistent language around the bets that they're making and the inputs and outputs, something that transcends one particular effort or feature. And that's powerful because if you think about things like annual strategy reviews or quarterly reviews (or kickoffs or retrospectives), it's really important to have a common thread between them. That is a hallmark of teams that appear to have a healthy perspective on using data.

---

## Functional Feature Factories

I have spoken to many companies, especially for business-to-business software, that are what I could most aptly describe as high-functioning feature factories. I would divide them into three categories.

Imagine you have a company that can't get anything done—or if it gets anything done, it's completely unusable. They're always chasing silver bullets. Really, nothing is clicking.

Then you have what I call "functional feature factories." They release reasonably usable features. Customers are grateful ("That that was something we asked for,") and they are not obvious duds. They keep chipping away at what they're doing. The main thing that defines these companies is not that they're doing terribly, but rather the lack of serious focus and step changes in their product, things that really help their customers do their job a lot better. The price points aren't all that high, so people will churn if it's not really providing that extra special value.

The third type of company is one that really nails high decision quality and high decision velocity by limiting the complexity they're adding to their product in relation to the outcomes that they're creating for customers.

In the middle category, they have an impact level of 4 to 7 on a scale of 1 to 10; they're chugging along. Sometimes they land a dud that's a 1 or a 2, but mostly they're in the range of 4 to 6. But they don't really have those extended 10s. They don't knock it out of the park repeatedly or in a disciplined way.

---

## Teams Closer to Customers

One question I ask teams is, "How long does it take to get a customer or user on the phone?" The amazing thing with that question is the incredibly wide spectrum of answers I get. This can range from "We can just pick up the phone at any time." With some products, if you can introduce in-app prompts or whatever, it really is only a couple of minutes before you can get a real user that matches particular criteria connected with the team. It ranges from that fast to months (and many hoops to jump through).

One important consideration here is that people often talk about bringing problems to the team. And my particular perspective is, problems have all kinds of interpretations, and even the best analysts and the best people at understanding problem can bias particular problems as they drop them on the team.

So the real question is: can you connect your teams with the actual human beings that are expected to get value out of what you are doing? And at a minimum, do teams have really rich behavioral data about how people are using the product, to augment these kind of human conversations that they're having?

This relates to product data and product intelligence. Someone would say, "We can connect directly with our customers, but we're not really sure we need that." What you find is that these things work wonderfully in tandem when you're using the behavioral data at scale to narrow down to the specific moments, the specific people to specific things that are interesting to you—and then reaching out to those people to understand the why and the backstory behind those things.

I think the real question is: are you bringing the customer and the user close to the team? Not just the problem, but are you bringing the person with the problem close to the team so that they can interact and connect? And how many proxies are you introducing between the team of problem solvers and this human being with a problem?

---

## One Pagers

A lot of companies now do some kind of one pager. The idea of a one pager is that you communicate the bet, the thing that you have in mind, in one page. The challenge is that imposing one pagers on the team in a certain format can only reflect a specific type of bet. Providing certain pieces of information in a certain way all the time, for example, solution estimates, etc. can close you off to the variety of bets that people actually actually want to try to execute on.

What you want is something that's flexible. I find myself recommending a checklist for teams. It's used as a reminder (but not a requirement) that prompts them to include those things. A great example is when you have a more open-ended opportunity-based bet. If you have a bet that's more opportunity-focused, you see the opportunity, and you trust that the team will be able to have leverage against that opportunity. You don't want to try to pre-converge and figure out that particular solution. You're proposing that the opportunity is there, and you might even propose a series of stopping functions, a series of pivot-or-proceed points as you try to exploit that opportunity.

If you have a one pager format that requires someone to provide an estimate or a level of effort measure or any number of things like that, you're going to discourage that person from proposing that particular bet. You're going to force them to come up with stuff just so that it can fit the format, which is not helpful.

---

## Leaving Time to Iterate

One pain point I hear extremely often is that people will remark that they never have time to iterate on the work that they've done. This is repeated by product managers, engineers, designers—everyone. So the question boils down to, "How do we leave an opportunity iterate?"

If you don't leave that opportunity and don't expect to get that opportunity, what you find is that teams are extremely hesitant to think about their work in a more incremental, iterative way. They're scared because they're worried that once they say it's done, they won't really get an opportunity to go back and work on it.

One of the tricks is finding the perfect balance when you're framing a mission and leaving enough flexibility to iterate. But also iterating with the confidence of the organization, the confidence of those people around you that you're not just going to iterate frivolously for a year or until you're happy with it. There must be confidence that you're going to have a bias for action, and you're not going to gold plate it.

Specifically, that might look like, "We're going to iterate on this until we can get that metric, which we believe to be a leading indicator of success for this until we can get it to this point," or, "We're going to continue iterating until we have a much higher confidence that this is true," for something that you're hoping to learn about customers. We're going to reflect on this every week. We're going to reflect on our current confidence about that number, and we're going to make a decision.

---

## PMs Creating an Environment for Good Decisions

One model that I like to use it, which is not original to me, is to ask the product manager, "Are you contributing to an environment where the best decisions can happen reasonably quickly?"

This challenges some product managers because they perceive it to be their particular role to make decisions. They are the decider or they are the idea person, or they're the person expected to have all the answers.

What you find when you're the sole decider or the sole idea person is that your ideas often aren't the best ideas and become the single point of failure. If you're not there, or if you're distracted or doing something else, the team depends on you to make these decisions because you haven't created adequate context for everyone to make these decisions independently.

There are some environments where they're actually really good decisions. They just don't happen anywhere nearly as quickly as they need to happen. You can spend a long time making a perfect decision, but by that point, often the ship has sailed and the extra time you spent trying to get a perfect decision was not really worth it.

You also see organizations that make decisions incredibly quickly, but they don't make particularly good decisions. You're trying to find that balance in what you're doing. There are a lot of things that contribute to making great decisions. You need information. You need to be able to interpret that information. You need to make models and frameworks that help de-bias what you're doing and bring in more perspectives. You need to create flow on the team. You need to work on feedback loops. You can't make great decisions without these feedback loops.

---

## Two Priority Levels

I was joking recently that you could go reasonably far with just two priority levels of work. One extreme is that you believe that the opportunity is extremely valuable and that the effort is experimentation-friendly. In this case it's something that you can chip away at, you can experiment, or the thing is reasonably valuable and it's guaranteed to be really, really small.

If you look at the work the broader team has in progress—the 60 teams or 30 teams or 15 teams—and you ask those people what the potential value of the things they're working on is, you often see this massive spectrum. Team one's highest priority thing is 10 times the potential value of team two's highest priority thing.

What you realize is that often we talk ourselves into doing things that are of medium value, something that seems pretty good. We've got some data points on it. Oh, and it's reasonably small. You can see the thought process behind it. Often, someone wants it, or we think the issue causes customers a little bit of pain, so it's so easy to talk ourselves into those things because they're not ridiculously low-value, but still…only medium value. The danger when that happens is that we miss out on those things which are a step change, which would be more valuable to explore.

---

## Artificial Deadlines

I get a lot of questions about deadlines: should we set deadlines and are deadlines important? The first question I ask is, "Is it a real deadline, or is it an artificial deadline?" And that can confuse people. What am I getting at? A real deadline is something where the value of the thing dramatically falls if the deadline is missed.

A lot of deadlines are artificial. And this is why I recommend that product managers get out of the business of artificial deadlines. Unless they admit that they are artificial, they're really putting their credibility with the team on the line.

An artificial deadline might arise in a situation where the value might drop if you quote/unquote "miss" the artificial deadline. But it's not going to drop precipitously right after that particular point. A better alternative at that point is to make it very clear about your hypothesis, about how the value will decay—what some people call the cost of delay for an item. What would it cost if we were a week late? What would it cost us if we were a month late?

The general advice that I tried to give is: stay away from artificial deadlines when there are real deadlines. Be incredibly forthright about them, and you're going to have to be open to the idea that scope will be variable at that point, because if you're going to hit that date, you know something will to have to give.

---

## Forcing Functions Are Valuable

I like to really make sure that teams understand the concept and value of healthy forcing functions. One really good example of this: our sprints are often misunderstood, and teams begin to resent them. Why are we working like that? It just seems like a tool used by management to get more out of us. And it's not really doing what we think it would do.

A time box is an example of a healthy forcing function, as is the idea is that you're going to circle back and integrate what you're doing and reflect on what you're doing as a team. Maybe exposing it to customers is an interesting forcing function or a variant of that forcing function.

There is a glossary of human computer interaction, and it gives the definition of a forcing function as something that's supposed to snap you out of automatic thought and force you to consider what's happening. I love that definition because the word "force" implies imposition and discomfort. But when used appropriately, what we're doing with forcing functions (for all of these different types) is to snap ourselves out of automatic thought and consider what's actually happening.

---

## Measurement to Inform Decisions

Many teams spend a lot of time obsessing about understanding what should they measure. And it's an interesting discussion because there's a lot baked into this particular discussion. Often they're wondering what their competitors are measuring. And in some ways too, they're scared. They're scared that they will measure the wrong thing—but not scared that they'll measure the wrong thing and maybe make the wrong decision. Certainly that's the case sometimes, but often they're worried that they will be graded or judged or penalized in their organization for somehow measuring the wrong thing.

But what you don't often hear, and what's important, is that what you measure should be related to the decisions that you need to make—where you see uncertainty and where you're willing to pay to reduce that uncertainty. That seems incredibly simple; why would you measure otherwise?

The real operative thing that I discuss with teams is: what are the big bets that are still open? What do you need to explore with this thing to in order to understand? Another question is, what's working right now, and what assumptions have you made that you want to ensure don't have an adverse impact? You want to make sure that they continue to work. So what do you need to learn? Where do you need to reduce uncertainty, and what do you want to make sure is working?

---

## The Reflection Is What Counts

In some businesses, you walk into work every day and see numbers move on a dashboard. You know that on that day, the 10 experiments that you're running did X, producing a certain amount of money for the business. That works in some cases, but that doesn't work in our particular environments. For us, the important part is that there is a level of reflection about these bets, that teams are talking about these things and following up on their particular experiments.

One is to make sure that when you're kicking off a particular effort, you're at least attempting to create some projections or forecasts about what you think will happen if you're successful or what the result will be. This could also be anticipating a likelihood of failure. If you do that at kickoff and you allow those things to be revisited periodically over the course of the effort, that gives you the blueprint to come back and reflect on the particular bet you made.

That's something that you just don't see a lot of. When you see teams, you see a lot of talk about what they're going to do, a lot of talk about the immediate release of the thing and celebrating it. But you don't see a lot of talk about following up on what you thought would happen. It's very good practice to put regular decision reviews and follow-ups into place. Do this in a disciplined way to ensure that you're staying honest with yourself.

---

## Shared Understanding / Vocabulary Is Super Hard

Every time you think that you've got shared understanding nailed, you don't. It turns out that there's more involved, more context that needs to be discussed. I mention this because you see a lot of the practices people put in place as a way to avoid the harder conversations. They try to isolate them, to have a smaller group of people have those conversations, to oversimplify what they're doing. But when you really boil it down, a lot of what we discuss needs more explanation.

I do an exercise with teams where I ask them to make a list of the words that they're having a tough time grappling with as a team. It's amazing to me because it turns out to be words like sprint, vision, results, outcomes, or MVP.

It's kind of funny that MVP (minimal viable product) comes up on the list of misunderstood words because it is understood in many different ways. What's fascinating is that people seem obsessed with trying to come up with a single definition of MVP instead of discussing the nuances in their particular environment that they're grappling with at the moment.

It's almost as if they've picked that particular phrase as a kind of focal point for debate. And the debate is what they should really be having. The discussion of the definition of MVP is secondary. This happens with a lot of these popular practices: it becomes about the practice way more than it becomes about the actual ideas.

---

## Products and Features Are Temporary

One thing you realize with a lot of software as a service products is that features, and maybe to a lesser extent products, are temporary. They are temporary delivery mechanisms to meet human needs and deliver some value or capability.

It's really important to keep this in mind: companies are constantly disrupting how they deliver the value. Something that's currently being done by a human being, and it takes a long time might be taken over or augmented by machine learning. Something that was really rough and involved a lot of steps is going to be disrupted. They're going to find a faster way to do it, or they can use a voice interface, or they're going to use any number of things.

The reason why this is really important is because with software products, we tend to get this idea that we're assembling the Mona Lisa, just in small parts, that there's this perfect problem product and we're just evolving it. We're just building it incrementally, and it exists out there in the future, it's just that we don't have anywhere near enough time. So we're going to build it in small parts.

What that doesn't take into account is that of the products that many of us love today, some have been around for a long time and are updated every year. Others have been augmented, maybe also updated every year, but now there's a developer ecosystem that we never anticipated. Maybe there is a template store that was never available before, or maybe we start paying for it as a subscription—whether we like it or not—when we used to just buy the update.

This is important for product development; it's our knowledge of the value exchange, our knowledge of what the customer values and how we can help them. That really persists over time, and it will persist even when the current delivery mechanism gets turned into something else.

---

**Source:** [50 Short Product Lessons](https://cutle.fish/blog/50-product-lessons) by John Cutler, June 29, 2020

_Note: This collection contains 50 comprehensive lessons. The lessons included here represent a selection of the most important insights from the full collection. For the complete set of all 50 lessons with full detail, please refer to the original source._


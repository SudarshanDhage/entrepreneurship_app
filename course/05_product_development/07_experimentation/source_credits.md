# Source Credits

## Experiments at Airbnb

- **Source:** The Airbnb Tech Blog (Medium)
- **Author:** Jan Overgoor
- **URL:** https://medium.com/airbnb-engineering/experiments-at-airbnb-e2db3abf39e7
- **Date:** 2025
- **Featured:** Jan Overgoor, Will Moss
- **Examples:** Price filter experiment, search page redesign, dummy A/A experiments

## Build vs Buy: Experimentation Platforms

- **Source:** Statsig Blog
- **Author:** Statsig
- **URL:** https://www.statsig.com/articles/build-vs-buy
- **Date:** Published on Statsig
- **Key Concepts:** Experimentation platforms, A/B testing infrastructure, build vs buy, total cost of ownership, opportunity cost, product experimentation, variant assignment, data processing, experiment analysis
- **Key Insight:** Framework for deciding whether to build an in-house experimentation platform or buy/rent one from external provider. Context: Companies like Bing, Google, LinkedIn increased experiments by order of magnitude after building platforms, run tens of thousands per year. Large tech companies have dedicated teams, but must decide based on business context. Two key questions: (1) What would it cost to build your own platform? (2) What can external platform deliver? Building your own requires investment in three areas: (1) Reliable core infrastructure to control and execute experiments, (2) Analysis framework to make sense of data and product decisions, (3) SDKs and tools to integrate with existing software development and decision-making processes. Core infrastructure has four components: (1) Experiment Control - persists and validates experiment specifications including iterations, parameters (start/end dates, control/variants, target users, sample sizes, configuration), enforces approval workflows, roles/permissions ensuring only approved experiments run, only authorized members start/stop. (2) Variant Assignment - decides how to assign user request to variant based on experiment specification and pseudo-random hash. Important considerations: random but deterministic assignment (users don't switch experiences), assigning users across multiple concurrent experiments ensuring consistent experience, keeping experiments isolated (one user not simultaneously in multiple conflicting experiments), delivering assignments without additional latency or hurting application availability. (3) Data Processing - reliable automated data processing critical for organizational trust. Steps: consolidate/sort/join logs from different sources, cleanse and enrich logs, compute metrics for segments/user dimensions, calculate p-values and confidence intervals. Critical: run A/A tests to ensure correctness, run basic health checks. (4) Experiment Analysis - enable everyone to easily read/interpret results. Separates valuable platform from one discarded quickly. Essential features: clear concise results highlighting relative change in metrics with statistical significance signals, must provide confidence intervals (platforms that don't are amusing at best, misleading/damaging at worst), reveal all relevant metrics not just experiment-tailored one, enable slicing across user dimensions/segments, use lower p-values to filter most significant metrics, per metric views of all experiment results to see most impactful experiments. APIs and SDKs: Experiment configurations must reach applications via APIs/SDKs. Considerations: support for client-side or server-side production decisioning, support for all languages teams choose, SDKs resilient to network flakiness, API versioning carefully considered with existing SDK usage. Buying/Renting considerations: (1) Core Functionality - delivers needed functionality? Optimized for front-end or back-end? Works for web and mobile? (2) SDK Support - supports SDK in needed language? (3) Performance - do SDK calls slow application? (4) Data Ingestion - can ingest from different sources to avoid logging with multiple services, join data for richer analysis? (5) Near Real-time Results - need near-real time to quickly detect/stop bad experiments? (6) Randomization Unit - allows randomization units like sessions/workspaces in addition to users? (7) Metrics - computes all required metrics? Do metrics reconcile with other company sources? (8) Health & Audit - automatically surfaces health/system metrics to proactively identify issues? Provides approval workflows for team reviews? Provides audit log of changes? (9) Access Control - allows controlling who has permissions to start/stop experiments? Building vs Buying: Estimate how many experiments organization would run assuming everyone embraces experimentation. As experiments scale, infrastructure becomes increasingly complex. To decide, consider TCO and opportunity cost of building in-house vs functional needs, reliability, scalability, costs of external service. Total Cost of Ownership for in-house: (1) Development - requires top-notch infrastructure engineers for high availability/performance, dedicated data scientists/data engineers for reliable trustworthy data processing/computations/results. (2) Operations & Support - team needs on-call rotation for uptime SLAs, support inquiries/tickets from all company users. (3) Infrastructure - must be redundant/resilient with automated scaling for stable trusted service, include monitoring/operational tooling, near-real time results increases complexity/cost further. (4) SDKs for Everyone - each team requires client/server SDKs in preferred language, platform team needs engineers familiar with language nuances to build/support SDKs over time. Opportunity Cost: Building in-house comes at expense of engineers/dollars that could invest in core product/functionality users want. Good experimentation platform accelerates innovation pace. If slows client teams, they won't use it and initial investment wasted. If decide to build, must invest in team/infrastructure/SDKs to stay ahead of all client teams' needs. External service advantage: Assuming external service meets functional requirements, buying may offer faster innovation pace than in-house. External services committed to innovating on behalf of all customers and setting industry standards for scalability, performance, security, governance. Means enjoy leading innovation pace in experimentation by default. When building might be better: Some cases building better if external services don't meet functional requirements. Example: Software deeply integrated with hardware (e.g., Roku) may need unique/customized way to deploy/control experiments. Summary: External service helps focus on driving car rather than assembling platform parts. For flexibility, consider service with free tier and easy onboarding to see value of simple A/B tests without contracts. Get something that works day one, most importantly provides transparent pricing so only get what need.
- **Used In:**
  - Chapter 5: Product Development (Experimentation) - Build vs Buy: Experimentation Platforms


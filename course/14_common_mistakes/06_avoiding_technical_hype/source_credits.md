# Source Credits

## Technical Hype and Cold Showers

- **Source:** Awesome Cold Showers (GitHub Repository)
- **Author/Curator:** Hillel Wayne (hwayne)
- **URL:** https://github.com/hwayne/awesome-cold-showers
- **Date:** Repository created and maintained (continually updated)
- **Key Concepts:** Technical hype, over-engineering, reality checks, microservices, big data systems, scaling, performance benchmarking, technical decision-making
- **Key Insight:** Curated collection of rigorous and respectful "cold showers" on overhyped technical topics. Premise: It's great when people get excited, but sometimes they get too excited - these are reminders to stay grounded. Not meant to dismiss enthusiasm, but to provide reality checks. For startups: Following technical hype can lead to over-engineering, premature optimization, choosing complex solutions for problems you don't have, wasting time/resources. Key examples: (1) Microservices - Five fallacies of why microservices solve problems monoliths have, shows either monoliths don't have those problems or microservices make them worse. Lesson: Start with monolith, microservices solve scaling/organizational problems you likely don't have yet, add significant operational complexity. (2) Scalability COST paper - Benchmarking graph-processing algorithms on 128-core clusters vs single-threaded 2014 Macbook Pro, laptop consistently wins (sometimes by order of magnitude). Lesson: "If using big data system for yourself, see if faster than your laptop. If building big data system for others, see it's faster than my laptop." Many startups over-invest in distributed systems when single server suffices. Scale when actually need to scale, not before. (3) Scaling SQLite to 4M QPS - Expensify found single bare-metal server faster and cheaper than x1e.32xlarge EC2 instance, avoided sharding complexity. Lesson: Don't assume need to scale out or use cloud services - sometimes single powerful server is faster/cheaper, especially avoiding sharding complexity. (4) Agile Methods critique - Review showing some practices (replacing requirements with user stories, lack of proper specification) harmful long run. Lesson: Agile good but imperfect, don't adopt dogma uncritically, some practices cause problems. (5) VM Warmup - Many benchmarks slow down over time, never stabilize, not repeatable due to non-determinism, warmup time important but usually ignored/reported inaccurately. Lesson: Don't trust performance claims without verifying in your context, many improvements don't hold up under realistic conditions. (6) Web Framework Benchmarks - Actual hard data of various combinations under various tasks. Lesson: Look for actual benchmark data not marketing claims, raw numbers in isolation don't tell whole story, need context and trade-offs. (7) Static vs Dynamic Typing - Review of literature showing solid research inconclusive about whether static typing reduces bugs, conclusive research had methodological issues. (8) Formal Verification - Literature review showing formal methods hard to learn, extremely expensive, often miss critical bugs. Three formally verified systems had critical correctness bugs (at system boundaries, not in protocols). (9) Identifier Naming - Research shows no difference in debugging time/quality between abbreviated vs full-word names. Eye-tracking suggests underscore may be processed faster than camelCase but research not definitive. (10) Go Concurrency - Despite claims Go's concurrency easier/less buggy, empirical study found plenty of concurrency bugs in popular Go software (Docker, Kubernetes, gRPC), more than half caused by Go-specific problems. Pattern: Question hype - do we actually have the problem this solves? What's actual evidence not just marketing? What are trade-offs/complexity costs? What if we started simpler? Stay grounded - best decisions based on understanding actual problems, evidence not hype, simplicity over complexity, incremental improvements over rewrites, measured results over isolated benchmarks. Key takeaways: Microservices often solve problems you don't have (start with monolith), big data systems may be slower than laptop (measure before assuming need distributed), scaling out isn't always better (single server can be faster/cheaper), Agile has problems (good but imperfect, don't adopt dogma), performance claims need verification (benchmarks misleading, test in context), question all hype (enthusiasm can lead astray), start simple (add complexity later if needed), measure don't assume (test actual solutions in context), evidence over marketing (rigorous studies not vendor claims), complexity has costs (every technology adds operational/cognitive overhead).
- **Used In:**
  - Chapter 14: Common Mistakes (Avoiding Technical Hype) - Technical Hype and Cold Showers

